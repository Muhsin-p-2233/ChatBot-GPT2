# -*- coding: utf-8 -*-
"""the_bot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSHwtJBqy_2XFUq5JhaB2wFEZZ07c8C4
"""

import pandas as pd
df=pd.read_csv("/content/drive/MyDrive/AI.csv")

df

import pandas as pd

# Path to your dataset
data_path = "/content/drive/MyDrive/AI.csv"

# Load the dataset
df = pd.read_csv(data_path)

# Preview the dataset
print(df.head())

"""Basic Data Summary"""

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/AI.csv")

# 1. Display the first few rows of the dataset
print("First few rows of the dataset:")
print(df.head())

# 2. Check the general structure of the dataset
print("\nDataset Info (including column types and non-null counts):")
print(df.info())

# 3. Get a summary of numeric columns
print("\nSummary statistics of numeric columns:")
print(df.describe())

# 4. Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# 5. Check the shape of the dataset (rows and columns)
print("\nShape of the dataset (rows, columns):")
print(df.shape)

# 6. Check the column names
print("\nColumn names in the dataset:")
print(df.columns)

"""Check for Missing Data"""

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/AI.csv")

# 1. Check for missing values in each column
missing_data = df.isnull().sum()

# 2. Display columns with missing values (if any)
print("Missing values in each column:")
print(missing_data)

# 3. Optionally, display columns with missing data greater than 0
missing_data = missing_data[missing_data > 0]
print("\nColumns with missing values:")
print(missing_data)

"""Check for Duplicate Data"""

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/AI.csv")

# 1. Check for duplicate rows
duplicate_data = df.duplicated().sum()

# 2. Print the number of duplicate rows
print(f"Number of duplicate rows: {duplicate_data}")

# 3. Optionally, if you want to see the duplicated rows:
duplicates = df[df.duplicated()]
print("\nDuplicate rows:")
print(duplicates)

"""Exploratory Data Analysis (EDA) on text data"""

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/AI.csv")

# 1. Calculate the length of each question and answer
df['question_length'] = df['Question'].apply(lambda x: len(str(x)))
df['answer_length'] = df['Answer'].apply(lambda x: len(str(x)))

# 2. Display basic statistics for question and answer lengths
print("Text length statistics:")
print(df[['question_length', 'answer_length']].describe())

# 3. Plot the distribution of text lengths
plt.figure(figsize=(12, 6))

# Plot question length distribution
plt.subplot(1, 2, 1)
df['question_length'].plot(kind='hist', bins=30, color='skyblue', edgecolor='black', alpha=0.7)
plt.title("Question Length Distribution")
plt.xlabel("Question Length")
plt.ylabel("Frequency")

# Plot answer length distribution
plt.subplot(1, 2, 2)
df['answer_length'].plot(kind='hist', bins=30, color='salmon', edgecolor='black', alpha=0.7)
plt.title("Answer Length Distribution")
plt.xlabel("Answer Length")
plt.ylabel("Frequency")

# Show the plots
plt.tight_layout()
plt.show()

""" Visualizing Common Words (Word Cloud)"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine all questions and answers
all_text = ' '.join(df['Question']) + ' ' + ' '.join(df['Answer'])

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Plot word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud of Questions and Answers")
plt.show()

"""Scatter Plot for Relationships (Text Lengths or Other Features)"""

# Scatter plot of question length vs. answer length
plt.figure(figsize=(8, 6))
plt.scatter(df['question_length'], df['answer_length'], alpha=0.5)
plt.title("Scatter Plot: Question Length vs Answer Length")
plt.xlabel("Question Length")
plt.ylabel("Answer Length")
plt.show()

"""Text Analysis - Word Frequency for Relationships"""

from collections import Counter

# Split questions and answers into words
question_words = ' '.join(df['Question']).split()
answer_words = ' '.join(df['Answer']).split()

# Get word frequency counts
question_word_counts = Counter(question_words)
answer_word_counts = Counter(answer_words)

# Display the most common words in questions and answers
print("Most Common Words in Questions:", question_word_counts.most_common(10))
print("Most Common Words in Answers:", answer_word_counts.most_common(10))

"""Visualizing Categorical Relationships"""

import seaborn as sns
import matplotlib.pyplot as plt

# Box plot to compare question lengths across categories (if 'category' column exists)
sns.boxplot(x='Question', y='question_length', data=df)
plt.title("Boxplot: Question Length by Category")
plt.xticks(rotation=90)
plt.show()

# Box plot to compare answer lengths across categories (if 'category' column exists)
sns.boxplot(x='Answer', y='answer_length', data=df)
plt.title("Boxplot: Answer Length by Category")
plt.xticks(rotation=90)
plt.show()

print(df[['question_length', 'answer_length']].describe())

!pip install datasets

from datasets import Dataset
from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
import pandas as pd

# Load the new dataset (AI.csv)
df = pd.read_csv("/content/drive/MyDrive/AI.csv")

# Ensure 'combined' column is created correctly (Q&A combined format)
df['combined'] = "Q: " + df['Question'] + " A: " + df['Answer']

# Filter out rows with empty or None values in the 'combined' column
df = df[df['combined'].notna() & (df['combined'] != '')]

# Convert the data into a Hugging Face Dataset format
qa_dataset = Dataset.from_pandas(df[['combined']])

# Initialize the tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos token

# Tokenize function
def tokenize_function(examples):
    # Tokenizing the examples and creating labels by shifting the input_ids
    encodings = tokenizer(examples['combined'], padding=True, truncation=True, max_length=128)
    # Shifting input_ids to create the labels
    encodings['labels'] = encodings['input_ids'].copy()
    return encodings

# Tokenize the dataset
tokenized_datasets = qa_dataset.map(tokenize_function, batched=True)

# Load the pre-trained GPT-2 model
model = GPT2LMHeadModel.from_pretrained(model_name)
model.config.pad_token_id = tokenizer.pad_token_id  # Set padding token for model

# Split the dataset into train and validation sets
train_test_split = tokenized_datasets.train_test_split(test_size=0.1)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2_finetuned",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",  # Avoids wandb logging unless required
)

# Data collator for the GPT-2 model
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# Start training
trainer.train()

import torch
import psutil
print(torch.cuda.memory_allocated())  # Total allocated memory on the GPU
print(torch.cuda.memory_reserved())   # Total reserved memory on the GPU
print(torch.cuda.memory_cached())     # Total cached memory (for older versions of PyTorch)
print(f"Memory Usage: {psutil.virtual_memory().percent}%")

"""Evaluate the Model"""

# Evaluate the model on the validation set
eval_results = trainer.evaluate()
print("Evaluation Results:", eval_results)

"""Test the Model"""

def generate_response(question, model, tokenizer, max_length=50):
    # Prepare the input text
    input_text = f"Q: {question} A:"
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    # Ensure input_ids are on the same device as the model
    device = model.device
    input_ids = input_ids.to(device)

    # Generate response
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_beams=5,  # Beam search for better predictions
        early_stopping=True,
        pad_token_id=tokenizer.pad_token_id,
        attention_mask=input_ids.ne(tokenizer.pad_token_id)  # Explicit attention mask
    )

    # Decode and clean the output
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

# Example test
question = "What is Natural Language Processing (NLP)?"
response = generate_response(question, model, tokenizer)
print(f"Q: {question}")
print(f"A: {response}")

"""Save the Model"""

model.save_pretrained("./gpt2_finetuned_model")
tokenizer.save_pretrained("./gpt2_finetuned_model")

""" Load the Model for Deployment"""

# User Feedback Mechanism (Optional)
def collect_user_feedback(response, correct_answer):
    # Simulate user feedback
    print("Is the response correct? (yes/no)")
    user_feedback = input(f"Response: {response}\nYour answer: {correct_answer}\nYour feedback: ")

    if user_feedback.lower() == 'yes':
        return 1
    else:
        return 0

# Function to calculate accuracy based on feedback
def calculate_accuracy(predictions, correct_answers):
    correct = 0
    total = len(predictions)
    for pred, correct_ans in zip(predictions, correct_answers):
        correct += collect_user_feedback(pred, correct_ans)
    return correct / total * 100

# Test the accuracy with sample queries
sample_questions = ["What is AI?", "What is Deep Learning?", "What is NLP?"]
sample_answers = ["AI is Artificial Intelligence.", "Deep Learning is a subset of machine learning.", "Natural Language Processing is about teaching machines to understand human language."]
predictions = [generate_response(q, model, tokenizer) for q in sample_questions]
accuracy = calculate_accuracy(predictions, sample_answers)
print(f"Chatbot Accuracy: {accuracy}%")

from transformers import GPT2LMHeadModel, AutoTokenizer

# Load the fine-tuned model and tokenizer
model = GPT2LMHeadModel.from_pretrained("./gpt2_finetuned_model")
tokenizer = AutoTokenizer.from_pretrained("./gpt2_finetuned_model")

# Test loading
question = "What is Natural Language Processing (NLP)?"
response = generate_response(question, model, tokenizer)
print(f"Q: {question}")
print(f"A: {response}")